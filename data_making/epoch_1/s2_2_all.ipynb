{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bfbd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from vae_earlystopping import EarlyStopping\n",
    "from model.m2_bce import BCEcVAE\n",
    "from model.m2_mse import MSEcVAE\n",
    "from loss.l2_bce import l2_bce\n",
    "from loss.l2_mse import l2_mse\n",
    "import joblib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "### 구하기\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4fd692",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"random_state\": [],\n",
    "    \"R2_BINARY\": [],      # bce_binary * mse (최종)\n",
    "    \"R2_BCE_MSE\": [],     # bce_prob * mse (sigmoid 가중)\n",
    "    \"R2_MSE\": []          # mse만 (x_hat_fin)\n",
    "}\n",
    "for i in np.random.randint(1,100,size = 20):\n",
    "    x_data = np.load('./data/metal.npy')\n",
    "    c_data = np.load('./data/pre_re.npy')\n",
    "    x_train,x_test,c_train,c_test = train_test_split(x_data,c_data, random_state = i,test_size = 0.4)\n",
    "    x_val,x_test,c_val,c_test = train_test_split(x_test,c_test,random_state = i, test_size = 0.5)\n",
    "    x_scaler = MinMaxScaler()\n",
    "    c_scaler = MinMaxScaler()\n",
    "    x_train = x_scaler.fit_transform(x_train)\n",
    "    c_train = c_scaler.fit_transform(c_train)\n",
    "    x_val,x_test = [x_scaler.transform(x) for x in [x_val,x_test]]\n",
    "    c_val,c_test = [c_scaler.transform(c) for c in [c_val,c_test]]\n",
    "\n",
    "    x_train,x_val,x_test = [torch.tensor(x, dtype = torch.float32) for x in [x_train,x_val,x_test]]\n",
    "    c_train,c_val,c_test = [torch.tensor(c, dtype = torch.float32) for c in [c_train,c_val,c_test]]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_data = [x_train,c_train]\n",
    "    val_data = [x_val,c_val]\n",
    "    test_data = [x_test,c_test]\n",
    "    train_data = TensorDataset(*train_data)\n",
    "    val_data = TensorDataset(*val_data)\n",
    "    test_data = TensorDataset(*test_data)\n",
    "    datas = [train_data,val_data,test_data]\n",
    "    train_loader,val_loader,test_loader = [DataLoader(x,batch_size = 64,shuffle=False) for x in datas]\n",
    "    x_sample,c_sample = next(iter(train_loader))\n",
    "\n",
    "    x_dim = x_sample.shape[1]\n",
    "    c_dim = c_sample.shape[1]\n",
    "    x_dim,c_dim\n",
    "\n",
    "    ### BCE모델에 대해서 정함\n",
    "    model = BCEcVAE(x_dim,c_dim,z_dim=8).to(device)\n",
    "    early_stopping = EarlyStopping(patience=40,min_delta = 1e-9)\n",
    "    optimizer = optim.Adam(model.parameters(),lr = 1e-3, weight_decay=1e-5)\n",
    "    epochs = 600\n",
    "    ### train_val loader에서의 학습\n",
    "    for epoch in range(1,epochs+1):\n",
    "        model.train()\n",
    "        t_loss= 0\n",
    "        for x,c in train_loader:\n",
    "            x,c = x.to(device),c.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            bce_logit, mu, logvar = model(x,c)\n",
    "            loss_dict = l2_bce(bce_logit, x,mu,logvar)\n",
    "            loss_dict['loss'].backward()\n",
    "            optimizer.step()\n",
    "            t_loss +=loss_dict['loss'].item()\n",
    "        model.eval()\n",
    "        v_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for v_x, v_c in val_loader:\n",
    "                v_x,v_c = v_x.to(device),v_c.to(device)\n",
    "                v_bce_logit, v_mu, v_logvar = model(v_x,v_c)\n",
    "                loss_dict = l2_bce(v_bce_logit, v_x, v_mu,v_logvar)\n",
    "                v_loss += loss_dict['loss'].item()\n",
    "        avg_train_loss = t_loss/len(train_loader)\n",
    "        avg_val_loss = v_loss/len(val_loader)\n",
    "\n",
    "        if epoch % 20 ==0  or epoch ==2:\n",
    "            print(f'Epoch [{epoch}/{epochs}]|Train:{avg_train_loss:.4f} |Val:{avg_val_loss:.4f}')\n",
    "        if early_stopping(avg_val_loss,model):\n",
    "            break\n",
    "\n",
    "    from bce_metrics.bce_solve import eval_bce_metrics\n",
    "    early_stopping.load_best_model(model)\n",
    "    model.eval()\n",
    "    all_x_hat = []\n",
    "    all_x_true = []\n",
    "    with torch.no_grad():\n",
    "        for x_t, c_t in test_loader:\n",
    "            x_t,c_t = x_t.to(device),c_t.to(device)\n",
    "            bce_logit_t,mu_t,logvar_t = model(x_t,c_t)\n",
    "            x_true = (x_t>0).float()\n",
    "            all_x_hat.append(bce_logit_t.detach().cpu())\n",
    "            all_x_true.append(x_true.detach().cpu())\n",
    "    all_x_hat_1 = torch.cat([a.flatten() for a in all_x_hat])\n",
    "    all_x_true = torch.cat([a.flatten() for a in all_x_true])\n",
    "\n",
    "    metrics = eval_bce_metrics(all_x_hat_1,all_x_true,threshold=0.5)\n",
    "    metrics\n",
    "    ### MSE 구하는 방법\n",
    "    x_sample,c_sample = next(iter(train_loader))\n",
    "    x_dim = x_sample.shape[1]\n",
    "    c_dim = c_sample.shape[1]\n",
    "    x_dim,c_dim\n",
    "    model = MSEcVAE(x_dim,c_dim,z_dim=8).to(device)\n",
    "    early_stopping = EarlyStopping(patience=40,min_delta = 1e-9)\n",
    "    optimizer = optim.Adam(model.parameters(),lr = 1e-3, weight_decay=1e-5)\n",
    "    epochs = 800\n",
    "    for epoch in range(1,epochs+1):\n",
    "        model.train()\n",
    "        t_loss = 0\n",
    "        for x,c in train_loader:\n",
    "            x,c = x.to(device),c.to(device)\n",
    "            x_hat,mu,logvar = model(x,c)\n",
    "            loss_dict = l2_mse(x_hat,x,mu,logvar)\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict['loss'].backward()\n",
    "            optimizer.step()\n",
    "            t_loss += loss_dict['loss'].item()\n",
    "        v_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for v_x,v_c in val_loader:\n",
    "                v_x,v_c = v_x.to(device),v_c.to(device)\n",
    "                x_hat,v_mu,v_logvar = model(v_x,v_c)\n",
    "                loss_dict = l2_mse(x_hat,v_x,v_mu,v_logvar)\n",
    "                v_loss += loss_dict['loss'].item()\n",
    "        avg_train_loss = t_loss/len(train_loader)\n",
    "        avg_val_loss = v_loss/len(val_loader)\n",
    "\n",
    "        if epoch % 20 ==0  or epoch ==2:\n",
    "            print(f'Epoch [{epoch}/{epochs}]|Train:{avg_train_loss:.4f} |Val:{avg_val_loss:.4f}')\n",
    "        if early_stopping(avg_val_loss,model):\n",
    "            break\n",
    "    early_stopping.load_best_model(model)\n",
    "    model.eval()\n",
    "    mse_logit_list = []\n",
    "    x_true_list = []\n",
    "    with torch.no_grad():\n",
    "        for x_t, c_t in test_loader:\n",
    "            x_t,c_t = x_t.to(device),c_t.to(device)\n",
    "            x_hat,mu_t,logvar_t = model(x_t,c_t)\n",
    "            mse_logit_list.append(x_hat.cpu().numpy())\n",
    "            x_true_list.append(x_t.cpu().numpy())\n",
    "    mse_logits = np.vstack(mse_logit_list)\n",
    "    x_true = np.vstack(x_true_list)\n",
    "    ### BCE-> 0,1 표현하여 MSE에 곱하기\n",
    "    all_x_hat_tensor = torch.cat(all_x_hat, dim=0) # (Total_Samples, x_dim) 형태로 결합\n",
    "    bce_logits_np = all_x_hat_tensor.cpu().numpy()\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    bce_prob = sigmoid(bce_logits_np)\n",
    "    bce_binary = (bce_prob >= 0.5).astype(np.float32)\n",
    "    x_hat_fin = x_scaler.inverse_transform(mse_logits)\n",
    "    x_true = x_scaler.inverse_transform(x_true)\n",
    "    final_x_hat = x_hat_fin*bce_binary\n",
    "    final_x_sig = x_hat_fin*bce_prob\n",
    "    from sklearn.metrics import r2_score,mean_squared_error\n",
    "    r2_mse = r2_score(x_true.flatten(),x_hat_fin.flatten())\n",
    "    r2_bce_mse = r2_score(x_true.flatten(), final_x_hat.flatten())\n",
    "    r2_bce_mse_sig = r2_score(x_true.flatten(),final_x_sig.flatten())\n",
    "    ##각각의 r2저장\n",
    "    results[\"random_state\"].append(int(i))\n",
    "    results[\"R2_BINARY\"].append(float(r2_bce_mse))\n",
    "    results[\"R2_BCE_MSE\"].append(float(r2_bce_mse_sig))\n",
    "    results[\"R2_MSE\"].append(float(r2_mse))\n",
    "\n",
    "save_path = \"./results_r2.json\"\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved:\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
